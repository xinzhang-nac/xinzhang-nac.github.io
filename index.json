
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I’m currently a research scientist at AI and applied research in Meta, Inc. My work is about deep machine learning for recommendation system.\nI got my Ph.D. degree from the Dept. of Statistics at Iowa State University advised by Dr. Zhengyuan Zhu and Dr. Jia Liu. Before that, I obtained my B.S in Mathematics from Fudan University, Shanghai, China.\n","date":1670457600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1670457600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I’m currently a research scientist at AI and applied research in Meta, Inc. My work is about deep machine learning for recommendation system.\nI got my Ph.D. degree from the Dept.","tags":null,"title":"Xin Zhang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Xin Zhang","Jia Liu","Zhengyuan Zhu"],"categories":null,"content":"","date":1670457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670457600,"objectID":"5f0904c316433b5639ab8acc5087c056","permalink":"https://example.com/publication/journal-article/zhang2022learning/","publishdate":"2022-12-08T00:00:00Z","relpermalink":"/publication/journal-article/zhang2022learning/","section":"publication","summary":"Identifying the latent cluster structure based on model heterogeneity is a fundamental but challenging task arises in many machine learning applications. In this article, we study the clustered coefficient regression problem in the distributed network systems, where the data are locally collected and held by nodes.","tags":["Adaptive fused-lasso","Distributed generalized ADMM","Minimum spanning tree","MODIS","Sensornets"],"title":"Learning Coefficient Heterogeneity over Networks: A Distributed Spanning-Tree-Based Fused-Lasso Regression","type":"publication"},{"authors":["Zhuqing Liu","Xin Zhang","Prashant Khanduri","Songtao Lu","Jia Liu"],"categories":null,"content":"","date":1664755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664755200,"objectID":"95c55d2c1aca82a0a2830de5e64b0294","permalink":"https://example.com/publication/conference-paper/liu2022interact/","publishdate":"2022-10-03T00:00:00Z","relpermalink":"/publication/conference-paper/liu2022interact/","section":"publication","summary":"In recent years, decentralized bilevel optimization problems have received increasing attention in the networking and machine learning communities.However, for decentralized bilevel optimization over networks with limited computation and communication capabilities, how to achieve low sample and communication complexities are two fundamental challenges. In this paper, we make the first attempt to investigate the class of decentralized bilevel optimization problems with nonconvex and strongly-convex structure corresponding to the outer and inner subproblems, respectively. Our main contributions in this paper are two-fold: i) We first propose a deterministic algorithm called inner-gradient-descent-outer-tracked-gradient) that requires the sample complexity of $O}(n epsilon^{-1})$ and communication complexity of $O(epsilon^{-1})$ to solve the bilevel optimization problem, where $n$ and $epsilon \u003e 0$ are the number of samples at each agent and the desired stationarity gap, respectively. ii) To relax the need for full gradient evaluations in each iteration, we propose a stochastic variance-reduced version of INTERACT, which improves the sample complexity to $O(sqrt{n} epsilon^{-1})$ while achieving the same communication complexity as the deterministic algorithm. Our numerical experiments also corroborate our theoretical findings.","tags":["Decentralized Learning","Bilevel Optimization","Stochastic Optimization"],"title":"INTERACT: Achieving Low Sample and Communication Complexities in Decentralized Bilevel Learning over Networks","type":"publication"},{"authors":["Xin Zhang","Minghong Fang","Zhuqing Liu","Haibo Yang","Jia Liu","Zhengyuan Zhu"],"categories":null,"content":"","date":1664755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664755200,"objectID":"3c52440a3ed7f358354a7a509879ba3d","permalink":"https://example.com/publication/conference-paper/zhang2022net/","publishdate":"2022-10-03T00:00:00Z","relpermalink":"/publication/conference-paper/zhang2022net/","section":"publication","summary":"Federated learning (FL) has received a surge of interest in recent years thanks to its benefits in data privacy protection, efficient communication, and parallel data processing. Also, with appropriate algorithmic designs, one could achieve the desirable linear speedup for convergence effect in FL.However, most existing works on FL are limited to systems with i.i.d. data and centralized parameter servers and results on decentralized FL with heterogeneous datasets remains limited.Moreover, whether or not the linear speedup for convergence is achievable under fully decentralized FL with data heterogeneity remains an open question. In this paper, we address these challenges by proposing a new algorithm, called NET-FLEET, for fully decentralized FL systems with data heterogeneity. The key idea of our algorithm is to enhance the local update scheme in FL (originally intended for communication efficiency) by incorporating a recursive gradient correction technique to handle heterogeneous datasets. We show that, under appropriate parameter settings, the proposed NET-FLEET algorithm achieves a linear speedup for convergence. We further conduct extensive numerical experiments to evaluate the performance of the proposed NET-FLEET algorithm and verify our theoretical findings.","tags":["Decentralized Learning","Federated Learning","Stochastic Optimization"],"title":"NET-FLEET: Achieving linear convergence speedup for fully decentralized federated learning with heterogeneous data","type":"publication"},{"authors":["Zhuqing Liu","Xin Zhang","Jia Liu"],"categories":null,"content":"","date":1664755200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664755200,"objectID":"6bbee64d45d3b1d8cd687fb46e58ca85","permalink":"https://example.com/publication/conference-paper/liu2022synthesis/","publishdate":"2022-10-03T00:00:00Z","relpermalink":"/publication/conference-paper/liu2022synthesis/","section":"publication","summary":"To increase the training speed of distributed learning, recent years have witnessed a significant amount of interest in developing both synchronous and asynchronous distributed stochastic variance-reduced optimization methods. However, all existing synchronous and asynchronous distributed training algorithms suffer from various limitations in either convergence speed or implementation complexity. This motivates us to propose an algorithm called (semi-asynchronous path-integrated stochastic gradient search), which leverages the special structure of the variance-reduction framework to overcome the limitations of both synchronous and asynchronous distributed learning algorithms, while retaining their salient features. We consider two implementations of SYNTHESIS under distributed and shared memory architectures. We show that our SYNTHESIS algorithms have O(sqrt{N}epsilon^{-2}(Delta+1)+N) and O(sqrt{N}epsilon^{-2}(Delta+1) d+N) computational complexities for achieving an epsilon-stationary point in non-convex learning under distributed and shared memory architectures, respectively, where N denotes the total number of training samples and Delta represents the maximum delay of the workers.Moreover, we investigate the generalization performance of SYNTHESIS by establishing algorithmic stability bounds for quadratic strongly convex and non-convex optimization. We further conduct extensive numerical experiments to verify our theoretical findings.","tags":["Decentralized Learning","Asynchronous Distributed Learning","Stochastic Optimization","Variance Reduction","Gradient Delay"],"title":"SYNTHESIS: A semi-asynchronous path-integrated stochastic gradient method for distributed learning in computing clusters","type":"publication"},{"authors":["Haibo Yang","Zhuqing Liu","Xin Zhang","Jia Liu"],"categories":null,"content":"","date":1664668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664668800,"objectID":"eeb278d2abaa342a954ebbe7365c997a","permalink":"https://example.com/publication/conference-paper/yang2022sagda/","publishdate":"2022-10-02T00:00:00Z","relpermalink":"/publication/conference-paper/yang2022sagda/","section":"publication","summary":"Federated min-max learning has received increasing attention in recent years thanks to its wide range of applications in various learning paradigms. In this paper, we propose a new algorithmic framework called stochastic sampling averaging gradient descent ascent (SAGDA), which yields an O(ε−2) communication complexity that is orders of magnitude lower than the state of the art.","tags":["Federated Learning","MiniMax Optimization","Stochastic Optimization"],"title":"SAGDA: Achieving O(ε−2) Communication Complexity in Federated Min-Max Learning","type":"publication"},{"authors":["Haibo Yang","Xin Zhang","Prashant Khanduri","Jia Liu"],"categories":null,"content":"","date":1656374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656374400,"objectID":"5ef132bb861720e8be4e34f4e39395c7","permalink":"https://example.com/publication/conference-paper/yang2022anarchic/","publishdate":"2022-06-28T00:00:00Z","relpermalink":"/publication/conference-paper/yang2022anarchic/","section":"publication","summary":"Present-day federated learning (FL) systems de- ployed over edge networks consists of a large number of workers with high degrees of heterogeneity in data and/or computing capabilities, which call for flexible worker participation in terms of timing, effort, data heterogeneity, etc. To satisfy the need for flexible worker participation, we consider a new FL paradigm called “Anarchic Federated Learning” (AFL) in this pa- per. In stark contrast to conventional FL models, each worker in AFL has the freedom to choose i) when to participate in FL, and ii) the number of local steps to perform in each round based on its current situation (e.g., battery level, commu- nication channels, privacy concerns). However, such chaotic worker behaviors in AFL impose many new open questions in algorithm design. In particular, it remains unclear whether one could develop convergent AFL training algorithms, and if yes, under what conditions and how fast the achievable convergence speed is. Toward this end, we propose two Anarchic Federated Aver- aging (AFA) algorithms with two-sided learning rates for both cross-device and cross-silo settings, which are named AFA-CD and AFA-CS, respectively. Somewhat surprisingly, we show that, un- der mild anarchic assumptions, both AFL algorithms achieve the best known convergence rate as the state-of-the-art algorithms for conventional FL. Moreover, they retain the highly desirable linear speedup effect with respect of both the number of workers and local steps in the new AFL paradigm. We validate the proposed algorithms with extensive experiments on real-world datasets.","tags":["Federated Learning","Asynchronous Optimization","Stochastic Gradient Descent"],"title":"Anarchic Federated Learning","type":"publication"},{"authors":["Weidong Liu","Xiaojun Mao","Xin Zhang"],"categories":null,"content":"","date":1643241600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1643241600,"objectID":"80bf405f6098d89b25aa029008114280","permalink":"https://example.com/publication/journal-article/liu2022fast/","publishdate":"2022-01-27T00:00:00Z","relpermalink":"/publication/journal-article/liu2022fast/","section":"publication","summary":"Decentralized sparsity learning has attracted a significant amount of attention recently due to its rapidly growing applications. In this paper, we proposed a decentralized surrogate median regression (deSMR) method for efficiently solving the decentralized sparsity learning problem.","tags":["Decentralized Optimization","Sparse Learning","Robust Regression"],"title":"Fast and Robust Sparsity Learning over Networks: A Decentralized Surrogate Median Regression Approach","type":"publication"},{"authors":["Xin Zhang","Zhuqing Liu","Jia Liu","Zhengyuan Zhu","Songtao Lu"],"categories":null,"content":"","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"b65e40088c4fa57ddcaf7c6e89817bee","permalink":"https://example.com/publication/conference-paper/zhang2021taming/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/publication/conference-paper/zhang2021taming/","section":"publication","summary":"Cooperative multi-agent reinforcement learning (MARL) has found many scientific and engineering applications. In this paper, we focus on decentralized MARL policy evaluation with nonlinear function approximation and propose efficient optimization algorithms.","tags":["Multi-agent Reinforcement Learning","Decentralized MiniMax Optimization","Stochastic Optimization","Variance Reduction"],"title":"Taming Communication and Sample Complexities in Decentralized Policy Evaluation for Cooperative Multi-Agent Reinforcement Learning","type":"publication"},{"authors":["Xin Zhang","Jia Liu","Zhengyuan Zhu","Elizabeth Serena Bentley"],"categories":null,"content":"","date":1627257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627257600,"objectID":"4a3e6293ac95f08d622eaf55aab92d66","permalink":"https://example.com/publication/conference-paper/zhang2021gt/","publishdate":"2021-07-26T00:00:00Z","relpermalink":"/publication/conference-paper/zhang2021gt/","section":"publication","summary":"Decentralized nonconvex optimization has received increasing attention in recent years in machine learning due to its advantages in system robustness, data privacy, and implementation simplicity. However, three fundamental challenges in designing decentralized optimization algorithms are how to reduce their sample, communication, and memory complexities. In this paper, we propose a gradient-tracking-based stochastic recursive momentum (GT-STORM) algorithm for efficiently solving nonconvex optimization problems. We show that to reach an $epsilon^2$-stationary solution, the total number of sample evaluations of our algorithm is $O(m^{1/2}epsilon^{-3})$ and the number of communication rounds is $O(m^{-1/2}epsilon^{-3})$, which improve the $O(epsilon^{-4})$ costs of sample evaluations and communications for the existing decentralized stochastic gradient algorithms. We conduct extensive experiments with a variety of learning models, including non-convex logistical regression and convolutional neural networks, to verify our theoretical findings. Collectively, our results contribute to the state of the art of theories and algorithms for decentralized network optimization","tags":["Decentralized Learning","Stochastic Optimization","Variance Reduction"],"title":"GT-STORM: Taming sample, communication, and memory complexities in decentralized non-convex learning","type":"publication"},{"authors":["Xin Zhang","Jia Liu","Zhengyuan Zhu","Elizabeth Serena Bentley"],"categories":null,"content":"","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620604800,"objectID":"c56d70c6037e762301ce1d0541720f49","permalink":"https://example.com/publication/conference-paper/zhang2021low/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/publication/conference-paper/zhang2021low/","section":"publication","summary":"Decentralized optimization has received increasing attention in recent years, due to its advantages in system robustness, data privacy and implementation simplicity. In this paper, we propose a decentralized hybrid stochastic gradient descent (DHSGD) algorithm for efficiently solving the nonconvex optimization problems in the decentralize fashion. We show that to reach an $epsilon^2$-stationary solution, the total sample complexity of our algorithm is $O(epsilon^{-3})$ and the communication complexity is $O(epsilon^{-3})$, which improves the $O(epsilon^{-4})$ sample and communication complexities of the existing decentralized stochastic gradient algorithms. We conduct extensive experiments with a variety of learning models to verify our theoretical findings. It is shown that our algorithm outperforms the existing methods when training on the large-scale datasets.","tags":["Decentralized Learning","Stochastic Optimization","Variance Reduction"],"title":"Low Sample and Communication Complexities in Decentralized Learning: A Triple Hybrid Approach","type":"publication"},{"authors":null,"categories":["Demo"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://example.com/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Xin Zhang","Jia Liu","Zhengyuan Zhu"],"categories":null,"content":"","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607040000,"objectID":"f8b363ced7055d405222c4b3cda36ab1","permalink":"https://example.com/publication/conference-paper/zhang2020taming/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"/publication/conference-paper/zhang2020taming/","section":"publication","summary":"Understanding the convergence performance of asynchronous stochastic gradient descent method (Async-SGD) has received increasing attention in recent years due to their foundational role in machine learning. To date, however, most of the existing works are restricted to either bounded gradient delays or convex settings. In this paper, we focus on Async-SGD and its variant Async-SGDI (which uses increasing batch size) for non-convex optimization problems with unbounded gradient delays. We prove o(1/ k) convergence rate for Async-SGD and o(1/k) for Async-SGDI. Also, a unifying sufficient assumption for Async-SGD’s convergence is proposed, which includes two major gradient delay models in the literature as special cases.","tags":["Asynchronous Distributed Learning","Stochastic Optimization","Non-Convex Learning","Gradient Delay"],"title":"Taming Convergence for Asynchronous Stochastic Gradient Descent with Unbounded Delay in Non-Convex Learning","type":"publication"},{"authors":null,"categories":["R"],"content":" R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart. ","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"bf1eb249db79f10ace7d22321494165a","permalink":"https://example.com/post/2020-12-01-r-rmarkdown/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/post/2020-12-01-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":null,"categories":null,"content":"Distribued learning algorithms work with the system formed by a group of computational nodes, each of which can locally store the data and pass the message in the system. The goal of these algorithm is to train a global learning model based on all nodes’ data through local computation and neighboring communication. Current distributed learning methods suffer from 1) high sample complexity due to the large local datasets; 2) inefficient communication caused by the low network bandwidth and complex model structure; 3) significant loss of data privacy when untrustable nodes exist; 4) unstable performance due to the asynchronous and byzantine nodes. Our work focuses on designing decentralized algorithms that can overcome these problems.\n","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"267ee4f74f194c413d3b99c5e3b18327","permalink":"https://example.com/project/distributed_learning/","publishdate":"2020-12-01T00:00:00Z","relpermalink":"/project/distributed_learning/","section":"project","summary":"We developed distributed algorithms for fast and efficient machine learning.","tags":["Machine Learning","Algorithm","Optimization"],"title":"Distributed Algorithms for Machine Learning ","type":"project"},{"authors":["Xin Zhang","Minghong Fang","Jia Liu","Zhengyuan Zhu"],"categories":null,"content":"","date":1602374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602374400,"objectID":"96c900a33db433ac82972a6b3d07e754","permalink":"https://example.com/publication/conference-paper/zhang2020private/","publishdate":"2020-10-11T00:00:00Z","relpermalink":"/publication/conference-paper/zhang2020private/","section":"publication","summary":"With rise of machine learning (ML) and the proliferation of smart mobile devices, recent years have witnessed a surge of interest in performing ML in wireless edge networks. In this paper, we consider the problem of jointly improving data privacy and communication efficiency of distributed edge learning, both of which are critical performance metrics in wireless edge network computing. Toward this end, we propose a new decentralized stochastic gradient method with sparse differential Gaussian-masked stochastic gradients (SDM-DSGD) for non-convex distributed edge learning. Our main contributions are three-fold: i) We propose a generalized differential-coded DSGD update, which enable a much lower transmit probability for gradient sparsification, and provide the convergence rate; ii) We theoretically establish the privacy and communication efficiency performance guarantee for our SDM-DSGD method, which outperforms all existing works; and iii) We reveal theoretical insights and offer practical design guidelines for the interactions between privacy preservation and communication efficiency, two conflicting performance goals. We conduct extensive experiments with a variety of learning models on MNIST and CIFAR-10 datasets to verify our theoretical findings. Collectively, our results contribute to the theory and algorithm design for distributed edge learning.","tags":["Decentralized Learning","Differential Privacy","Communication Efficient Optimization"],"title":"Private and communication-efficient edge learning: a sparse differential gaussian-masking distributed SGD approach","type":"publication"},{"authors":["Haibo Yang","Xin Zhang","Minghong Fang","Jia Liu"],"categories":null,"content":"","date":1588723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588723200,"objectID":"b03e0213856601f53173a3a4bf4d0531","permalink":"https://example.com/publication/conference-paper/yang2020adaptive/","publishdate":"2020-05-06T00:00:00Z","relpermalink":"/publication/conference-paper/yang2020adaptive/","section":"publication","summary":"In this work, we investigate a communication-efficient multi-hierarchical signSGD (MH-signSGD) algorithm with an adaptive learning rate. Under the symmetric assumption of the stochastic gradient distribution, we show that, without the need for learning rate tuning, our proposed MH-signSGD matches the state-of-art sublinear convergence rate $O(1/\\sqrt{K})$ in nonconvex settings, where $K$ is the number of iterations. Our adaptive learning strategy is based on stochastically approximating the learning rate found by greedily minimizing an error upper bound between two successive iterations. Moreover, by leveraging a normal approximation technique to characterize stochastic gradient sign error, we are able to sharpen the convergence analysis of MH-sighSGD with a fixed learning rate $1/\\sqrt{K}$ and establish a strong result in the large-system regime, which says that the MH-signSGD algorithm asymptotically converges to a stationary point at rate $O(1/\\sqrt{M})$, where $M$ is the number of workers. In comparison, most existing work on signSGD can only prove a weaker finite neighborhood convergence in the large system regime. We validate our theoretical results experimentally both on synthetic data and real-world datasets.","tags":["Distributed Learning","Message Compression","Stochastic Gradient Descent"],"title":"Adaptive multi-hierarchical signSGD for communication-efficient distributed optimization","type":"publication"},{"authors":["Haibo Yang","Xin Zhang","Minghong Fang","Jia Liu"],"categories":null,"content":"","date":1576022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576022400,"objectID":"a244bd671610f100625499282a55ffa5","permalink":"https://example.com/publication/conference-paper/yang2019byzantine/","publishdate":"2019-12-11T00:00:00Z","relpermalink":"/publication/conference-paper/yang2019byzantine/","section":"publication","summary":"In this work, we consider the resilience of distributed algorithms based on stochastic gradient descent (SGD) in distributed learning with potentially Byzantine attackers, who could send arbitrary information to the parameter server to disrupt the training process. Toward this end, we propose a new Lipschitz-inspired coordinate-wise median approach (LICM-SGD) to mitigate Byzantine attacks. We show that our LICM-SGD algorithm can resist up to half of the workers being Byzantine attackers, while still converging almost surely to a stationary region in non-convex settings. Also, our LICM- SGD method does not require any information about the number of attackers and the Lipschitz constant, which makes it attractive for practical implementations. Moreover, our LICM- SGD method enjoys the optimal O(md) computational time- complexity in the sense that the time-complexity is the same as that of the standard SGD under no attacks. We conduct extensive experiments to show that our LICM-SGD algorithm consistently outperforms existing methods in training multi-class logistic regression and convolutional neural networks with MNIST and CIFAR-10 datasets. In our experiments, LICM- SGD also achieves a much faster running time thanks to its low computational time-complexity.","tags":["Distributed Optimization","Byzantine-resilient Learning"],"title":"Byzantine-resilient stochastic gradient descent for distributed learning: A lipschitz-inspired coordinate-wise median approach","type":"publication"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you’ll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders …","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://example.com/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":["Xin Zhang","Jia Liu","Zhengyuan Zhu","Elizabeth S. Bentley"],"categories":null,"content":"","date":1556496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556496000,"objectID":"036bac57e3875e82ac428f57facc5fa2","permalink":"https://example.com/publication/conference-paper/zhang2019compressed/","publishdate":"2019-04-29T00:00:00Z","relpermalink":"/publication/conference-paper/zhang2019compressed/","section":"publication","summary":"Network consensus optimization has received in- creasing attention in recent years and has found important appli- cations in many scientific and engineering fields. To solve network consensus optimization problems, one of the most well-known approaches is the distributed gradient descent method (DGD). However, in networks with slow communication rates, DGD’s performance is unsatisfactory for solving high-dimensional net- work consensus problems due to the communication bottleneck. This motivates us to design a communication-efficient DGD- type algorithm based on compressed information exchanges. Our contributions in this paper are three-fold: i) We develop a communication-efficient algorithm called amplified-differential compression DGD (ADC-DGD) and show that it converges under any unbiased compression operator; ii) We rigorously prove the convergence performances of ADC-DGD and show that they match with those of DGD without compression; iii) We reveal an interesting phase transition phenomenon in the convergence speed of ADC-DGD. Collectively, our findings advance the state- of-the-art of network consensus optimization theory.","tags":["Decentralized Learning","Communication Efficient Optimizaiton","Message Compression"],"title":"Compressed distributed gradient descent: Communication-efficient consensus over networks","type":"publication"},{"authors":["Xin Zhang","Zhengyuan Zhu"],"categories":null,"content":"","date":1554422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554422400,"objectID":"60962fd5bfc29910f65f61fd19e24571","permalink":"https://example.com/publication/preprint/zhang2019spatial/","publishdate":"2019-04-05T00:00:00Z","relpermalink":"/publication/preprint/zhang2019spatial/","section":"publication","summary":"Detecting weak clustered signal in spatial data is important but challenging in applications such as medical image and epidemiology. A more efficient detection algorithm can provide more precise early warning, and effectively reduce the decision risk and cost. To date, many methods have been developed to detect signals with spatial structures. However, most of the existing methods are ei- ther too conservative for weak signals or computationally too intensive. In this paper, we consider a novel method named Spatial CUSUM (SCUSUM), which employs the idea of the CUSUM procedure and false discovery rate controlling. We develop theoretical properties of the method which indicates that asymptotically SCUSUM can reach high classification accuracy. In the simulation study, we demonstrate that SCUSUM is sensitive to weak spatial signals. This new method is applied to a real fMRI dataset as illustration, and more irregular weak spatial signals are detected in the images compared to some existing methods, including the conventional FDR, FDR_L and scan statistics.","tags":["Image Data Analysis","Anomaly Detection"],"title":"Spatial CUSUM for Signal Region Detection","type":"publication"},{"authors":["Xin Zhang"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post’s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://example.com/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Xin Zhang","Limin Li","Michael K.Ng","Shuqin Zhang"],"categories":null,"content":"Supplementary notes can be added here, including code, math, and images.\n","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"29b5890d989bfc81347aa85e4d4ab7d5","permalink":"https://example.com/publication/journal-article/zhang2017drug/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publication/journal-article/zhang2017drug/","section":"publication","summary":"Drug–target interaction (DTI) prediction is a challenging step in further drug repositioning, drug discovery and drug design. In this paper, a multiview DTI prediction method based on clustering is proposed. We first introduce a model for single view drug–target data. The model is formulated as an optimization problem, which aims to identify the clusters in both drug similarity network and target protein similarity network, and at the same time make the clusters with more known DTIs be connected together. Then the model is extended to multiview network data by maximizing the consistency of the clusters in each view. An approximation method is proposed to solve the optimization problem. We apply the proposed algorithms to two views of data. Comparisons with some existing algorithms show that the multiview DTI prediction algorithm can produce more accurate predictions. For the considered data set, we finally predict 54 possible DTIs. From the similarity analysis of the drugs/targets, enrichment analysis of DTIs and genes in each cluster, it is shown that the predicted DTIs have a high possibility to be true.","tags":["Biostatistics","Drug Discovery","Clustering Analysis"],"title":"Drug–target interaction prediction by integrating multiview network data","type":"publication"},{"authors":null,"categories":null,"content":"We build up the R package “FlightScanner” for Web scraping/crawling flight information from SkyScanner API (https://www.skyscanner.com/). This package can Keep track of flight price and help us choose best flights whie save money without spending too much time on searching.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"00058257a1026f9a6061ac162ad48aa3","permalink":"https://example.com/project/flightscanner/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/flightscanner/","section":"project","summary":"Course project for STAT 585. Web scraping/crawling flight information from SkyScanner API (https://www.skyscanner.com/).","tags":["Package","R"],"title":"R Package: FlightScanner","type":"project"},{"authors":["Xin Zhang","Weiguo Gao","Yun Su"],"categories":null,"content":"","date":1446336000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446336000,"objectID":"a7ae6b5fe7050dd19d722de3d65919c5","permalink":"https://example.com/publication/journal-article/xin2015electricity/","publishdate":"2015-11-01T00:00:00Z","relpermalink":"/publication/journal-article/xin2015electricity/","section":"publication","summary":"In this paper, a functional cluster method to analyze features of sparse and irregular longitudinal electricity data and to cluster all users is proposed. Firstly, kernel method is applied to estimate daily continuous curves of discrete data. Then, inspired by distance in Sobolev space, new distances for functional data usable in k-means algorithm are proposed. Based on experiment, electricity consumer archetypes for all users in different functional distances and cluster numbers are calculated. Result shows that, because users in experiment are mainly city residents, their consumption patterns are similar: big peak period is between June and September, small peak is between January and February, and consumption fluctuations aren't very intense. However, main difference is in consumption ranges: low-consumption user's daily consumptions are lower than 13 kW/h overall, while high-consumption users use almost 100 kW/h every day.","tags":["Electricity Data Analysis","Functional Clustering Analysis"],"title":"Electricity consumer archetypes study based on functional data analysis and K-means algorithm","type":"publication"},{"authors":["Xin Zhang","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"9d3f1d49d7aa2079477b733666df113f","permalink":"https://example.com/publication/journal-article/untitled-folder/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/untitled-folder/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Xin Zhang"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1428364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1428364800,"objectID":"769a5d3af86dc38ae1d6c46070e7ad30","permalink":"https://example.com/publication/preprint/_template/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/preprint/_template/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["Xin Zhang","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"54dfa68d971a80896cd939eef76bc512","permalink":"https://example.com/publication/conference-paper/untitled-folder/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/untitled-folder/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]